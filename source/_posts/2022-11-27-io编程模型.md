---
title: 主流应用io编程模型
date: 2022-11-27 23:44:21
tags:
- IO
- 编程
categories:
- IO
- 编程
---
## 基础知识
### 五中IO模型
* 阻塞IO
> 应用线程向内核请求数据时，如果没有数据**会阻塞该请求**，该线程陷入阻塞态，等数据到达时内核会将数据复制给应用空间，并唤醒线程，线程继续执行，返回成功
* 非阻塞IO
> 应用线程向内核请求数据时，如果没有数据线程不会陷入阻塞态，而是立即返回错误；应用线程再次请求数据时，如果有数据了，内核将数据拷贝到应用空间，返回成功
* IO多路复用
> 上面两种IO模型均需要一个线程服务一个连接，这将会造成非常大的资源消耗，因此一种新的IO模型被提了出来：使用一个线程专门负责监听连接，如果某连接可写或可读了
就通知应用，应用再分配对应的线程去读取/写入数据。比如Linux实现的select, poll, **epoll**这些IO工具。这样就能节省很多宝贵的线程资源。
* 信号驱动
> 背景：IO多路复用虽然解决了多线程开销大的问题，但应用程序还是需要轮询监听线程是否有可读/可写事件发生。
<!-- more -->
>当应用程序只用发起一次询问是否有可读/可写事件的请求(并通过系统调用sigaction执行一个信号处理函数，此时请求即刻返回)，当有可读可写事件发生时，内核通过信号回调通知应用进程，应用进程再分配对应的线程去读取/写入数据。
* 异步IO
> 背景：信号驱动虽然解决了应用需要轮询监听线程是否有可读/可写事件的问题，但还是需要发起一次询问内核是否有可读/可写事件的请求，然后才能调用read,write去读取/写入数据,
> 能不能有一种一劳永逸的方式，我只要发送一个请求我告诉内核我要读取数据，然后我就什么都不管了，然后内核去帮我去完成剩下的所有事情？
> 
> 有人设计了一种方案，应用只需要向内核发送一个read 请求,告诉内核它要读取数据后即刻返回；内核收到请求后会建立一个信号联系，当数据准备就绪，
> 内核会主动把数据从内核复制到用户空间，等所有操作都完成之后，内核会发起一个通知告诉应用，我们称这种一劳永逸的模式为异步IO模型。


#### 同步异步
* 同步：在IO模型里面如果请求方**从发起请求到数据最后完成的这一段过程中都需要自己参与**(过程中可能因socket不可读或不可写线程被阻塞，如果可能，称为同步阻塞IO，如果不可能，称为同步非阻塞IO)，那么这种我们称为同步请求；
* 异步：反之，如果应用发送完指令后就不再参与过程了，只需要等待最终完成结果的通知，那么这就属于异步。

## 基于epoll实现的IO编程模型
### go 
> 一个协程处理一个连接

一个简单的服务器
```azure
func main() {
 //构造一个listener
 listener, _ := net.Listen("tcp", "127.0.0.1:9008")
 for {
  //接收请求
  conn, err := listener.Accept()
  //启动一个协程来处理
  go process(conn)
 }
}

func process(conn net.Conn) {
 //结束时关闭连接
 defer conn.Close()
 //读取连接上的数据
 var buf [1024]byte
 len, err := conn.Read(buf[:])
 //发送数据
 _, err = conn.Write([]byte("I am server!"))

 ...
}
```
* go用户程序设置socket一般设置 **非阻塞** 模式，当调用go提供的accept()（会调用底层系统accept()）或read()时没有事件发生，会直接返回
  accept有个for循环，如果不可读，会park当前协程,并将当前协程关联到当前socket。
* 往后 Go scheduler 会在循环调度的 runtime.schedule() 函数以及 sysmon 监控线程中调用
  runtime.netpoll（epoll_wait） 以获取就绪的网络 socket 的文件描述符。根据网络就绪 fd 拿到 pollDesc。然后将对应的协程推入全局调度队列或者当前 P 本地调度队列去重新执行。此时协程被唤醒
  继续accept/read,这时将会读到socket缓冲区中的数据然后返回。
* 因为系统监控 Goroutine 直接运行在线程上，所以它获取的 Goroutine 列表会直接加入全局的运行队列，其他 Goroutine 获取的列表都会加入 Goroutine 所在处理器的运行队列上。
* 网络轮询器并不是由运行时中的某一个线程独立运行的，运行时的调度器和系统调用都会通过 runtime.netpoll 与网络轮询器交换消息，获取待执行的 Goroutine 列表，并将待执行的 Goroutine 加入运行队列等待处理。
* 所有的文件 I/O、网络 I/O 和计时器都是由网络轮询器管理的，它是 Go 语言运行时重要的组成部分。

### redis 
> 单线程， io-multiple reactor编程模型
* 单线程IO
![img_1.png](/images/io_code_model/img_1.png)
* 多线程IO
![img_2.png](/images/io_code_model/img_2.png)



### Nginx
> master-多worker 模式

1. 每个 Worker 都会有一个属于自己的 epoll 对象
2. 每个 Worker 会关注所有的 listen 状态上的新连接事件
3. 对于用户连接，只有一个 Worker 会处理，其它 Worker 不会持有该用户连接的 socket。
![img_4.png](/images/io_code_model/img_4.png)
![img_3.png](/images/io_code_model/img_3.png)
* Nginx 的 Master 中做的网络相关动作不多，仅仅只是创建了 socket、然后 bind 并 listen 了一下。接着就是用自己 fork 出来多个 Worker 进程来。由于每个进程都一样，所以每个 Worker 都有 Master 创建出来的 listen 状态的 socket 句柄。
* Worker 进程处理的网络相关工作就比较多了。epoll_create、epoll_ctl、epoll_wait 都是在 Worker 进程中执行的，也包括用户连接上的数据 read、处理 和 write。
![img_6.png](/images/io_code_model/img_6.png)
1.先是使用 epoll_create 创建一个 epoll 对象出来
2.设置回调为 ngx_event_accept
3.通过 epoll_ctl 将所有 listen 状态的 socket 的事件都管理起来
4.执行 epoll_wait 等待 listen socket 上的连接到来
5.新连接到来是 epoll_wait 返回，进入 ngx_event_accept 回调
6.ngx_event_accept 回调中将新连接也添加到 epoll 中进行管理（其回调为ngx_http_init_connection）
7.继续进入 epoll_wait 等待事件
8.用户数据请求到达时进入 http 回调函数进行处理



### netty
> 经典模式： boss-多worker模式

 ![img_5.png](/images/io_code_model/img_5.png)
* 在 Netty 中的 boss 线程中负责对父 channel（listen socket）上事件的监听和处理，当有新连接到达的时候，选择一个 worker 线程把这个子 channel（连接 socket ）交给 worker 线程来处理。

* 其中 Worker 线程就是等待其管理的所有子 channel（连接 socket）上的事件的监听和处理。当发现有事件发生的时候，回调用户设置的 handler 进行处理。在本文的例子中，这个用户 handler 就是 EchoServerHandler#channelRead。

### 总结
几乎所有的IO编程模型都是将socket设置成非阻塞模式，然后利用epoll（epoll本身是阻塞的）等IO多路复用进行事件监听，基于回调处理事件，特别的，go netpoll一个协程服务一个连接（协程内存占用小(2k)，Linux创建一个线程占用10M内存），上下文切换开销小（100ns,线程上下文切换是30us））

## 参考
https://zhuanlan.zhihu.com/p/144805500
https://zhuanlan.zhihu.com/p/115912936
https://zhuanlan.zhihu.com/p/541284978

## TODO

* 半包和闭包