---
title: epoll
date: 2022-11-27 16:38:59
tags:
- IO
categories:
- IO
---

### epoll
> 所熟知的 IO 多路复用机制。 这里的复用指的就是对进程的复用。
<!-- more -->
epoll简单例子
```c
#define MAX_EVENTS 10
           struct epoll_event ev, events[MAX_EVENTS];
           int listen_sock, conn_sock, nfds, epollfd;

           /* Code to set up listening socket, 'listen_sock',
              (socket(), bind(), listen()) omitted. */

           epollfd = epoll_create1(0);
           if (epollfd == -1) {
               perror("epoll_create1");
               exit(EXIT_FAILURE);
           }

           ev.events = EPOLLIN;
           ev.data.fd = listen_sock;
           if (epoll_ctl(epollfd, EPOLL_CTL_ADD, listen_sock, &ev) == -1) {
               perror("epoll_ctl: listen_sock");
               exit(EXIT_FAILURE);
           }

           for (;;) {
               nfds = epoll_wait(epollfd, events, MAX_EVENTS, -1);
               if (nfds == -1) {
                   perror("epoll_wait");
                   exit(EXIT_FAILURE);
               }

               for (n = 0; n < nfds; ++n) {
                   if (events[n].data.fd == listen_sock) {
                       conn_sock = accept(listen_sock,
                                          (struct sockaddr *) &addr, &addrlen);
                       if (conn_sock == -1) {
                           perror("accept");
                           exit(EXIT_FAILURE);
                       }
                       setnonblocking(conn_sock);
                       ev.events = EPOLLIN | EPOLLET;
                       ev.data.fd = conn_sock;
                       if (epoll_ctl(epollfd, EPOLL_CTL_ADD, conn_sock,
                                   &ev) == -1) {
                           perror("epoll_ctl: conn_sock");
                           exit(EXIT_FAILURE);
                       }
                   } else {
                       do_use_fd(events[n].data.fd);
                   }
               }
           }

}
```
1. accept 创建新 socket
   1. 初始化 struct socket 对象
   2. 为新 socket 对象申请 file
   3. 接收连接
   4. 添加新文件到当前进程的打开文件列表中
   
   socket 内核对象:
   ![img_20.png](/images/epoll/img_20.png)

2. epoll_create实现 
   1. eventpoll 这个结构体中的几个成员的含义如下：
       * wq： 等待队列链表。软中断数据就绪的时候会通过 wq 来找到阻塞在 epoll 对象上的用户进程。
       * rbr： 一棵红黑树。为了支持对海量连接的高效查找、插入和删除，eventpoll 内部使用了一棵红黑树。通过这棵树来管理用户进程下添加进来的所有 socket 连接。
       * rdllist： 就绪的描述符的链表。当有的连接就绪的时候，内核会把就绪的连接放到 rdllist 链表里。这样应用进程只需要判断链表就能找出就绪进程，而不用去遍历整棵树。


![img_19.png](/images/epoll/img_19.png)

3. epoll_ctrl添加socket

    > 假设我们现在和客户端们的多个连接的 socket 都创建好了，也创建好了 epoll 内核对象。在使用 epoll_ctl 注册每一个 socket 的时候，内核会做如下三件事情
    
    1. 分配一个红黑树节点对象 epitem，包含socket fd和epoll对象指针
    2. 添加等待事件到 socket 的等待队列中，其回调函数是 ep_poll_callback ,base指针指向epitem
    3. 将 epitem 插入到 epoll 对象的红黑树里
    epoll内存对象：
   ![img_17.png](/images/epoll/img_17.png)

4. epoll_wait 
   1. 判断就绪队列上有没有事件就绪 
   2. 定义等待事件并关联当前进程 
   3. 添加到等待队列 
   4. 让出CPU 主动进入睡眠状态
      * epoll本身是**阻塞**的，即调用epoll_wait()如果当前监听的所有文件描述符上没有事件发生（就绪队列（rdllist）为空），当前调用线程将会被阻塞
            epoll_wait 做的事情不复杂，当它被调用时它观察 eventpoll->rdllist 链表里有没有数据即可。有数据就返回，没有数据就创建一个等待队列项（包含当前线程和回调唤醒函数（default_wake_function）），
            将其添加到 eventpoll 的等待队列上，然后把自己阻塞掉就完事。
      ![img_18.png](/images/epoll/img_18.png)
5. 数据来啦
  > * socket->sock->sk_data_ready 设置的就绪处理函数是 sock_def_readable
  > * 在 socket 的等待队列项中，其回调函数是 ep_poll_callback。另外其 private 没有用了，指向的是空指针 null。
  > * 在 eventpoll 的等待队列项中，回调函数是 default_wake_function。其 private 指向的是等待该事件的用户进程。

  1. 接收数据到socket接收队列 
     1. 某个连接的数据来了，首先根据包上的src,dest信息获取对应的socket，然后将数据写入socket的接受队列中， 
  2. 查找就绪回调函数（ep_poll_callback） 
     1. 然后调用socket创建时设置的回调函数(sk->sk_data_ready = sock_def_readable)
         sock_def_readable 判断当前socket的等待队列是否为空，不为空就调用等待队列项的回调函数 ep_poll_callback 
  3. 执行 socket 就绪回调函数 
     1. 在ep_poll_callback 中首先根据等待队列项的额外的base指针找到当前socket的epitem,进而找到eventpoll对象，首先他做的第一件事就是将
     自己的epitem添加到epoll的就绪队列（rdllist）中,接着会查看当前epoll对象是否有等待项（epollwait()阻塞时会设置），如果有等待项，
       那就查找到等待项里设置的回调函数（default_wake_function)
  4. 执行 epoll 就绪通知 
     1. default_wake_function 会唤醒epollwait时被阻塞的线程

  从用户角度来看，**epoll_wait 只是多等了一会儿而已，但执行流程还是顺序的**。

#### 水平触发 边缘触发
>epoll有两种触发方式：EPOLLET /EPOLLLT,LT是默认方式
* EPOLLET（边缘触发）
  > 只有数据到来才触发，不管缓冲区中是否还有数据，缓存区中剩余为读完的数据不会导致epoll_wait()返回,**因此读事件发生时必须把数据读干净**；写事件同样的道理 
* EPOLLLT（水平触发）
  > 只要有数据都会触发，缓冲区中剩余未读尽的数据会导致epoll_wait()返回；缓冲区剩余空间未写完也会导致epoll_wait()返回，
    因此对于写事件一定要及时清除，避免不必要的触发，浪费CPU资源。
#### 总结
总结下，epoll 相关的函数里内核运行环境分两部分：

* 用户进程内核态。进行调用 epoll_wait 等函数时会将进程陷入内核态来执行。这部分代码负责查看接收队列，以及负责把当前进程阻塞掉，让出 CPU。
* 硬软中断上下文。在这些组件中，将包从网卡接收过来进行处理，然后放到 socket 的接收队列。对于 epoll 来说，再找到 socket 关联的 epitem，并把它添加到 epoll 对象的就绪链表中。 这个时候再捎带检查一下 epoll 上是否有被阻塞的进程，如果有唤醒之。
  ![img_16.png](/images/epoll/img_16.png)



### select poll epoll比较
#### select:
  * 单个进程打开的fd是有限制的，通过FD_SETSIZE设置，默认1024
  * 需要将被监听的fd列表从用户空间拷贝到内核空间
  * 轮询，复杂度o(max_fd + 1),例如fd列表为 1、2、1000， 复杂度为o(1000)
  * 每次调用select，需要重置fd列表对象中的是否可用字段
1. 基本原理
```azure
// Returns true if fd is ready for I/O.
bool is_ready(int fd);

struct fd_info {
  int fd;
  bool ready;
};

int select(set<fd_info> fds, int max_fd) {
  int ready_cnt = 0;
  while (ready_cnt == 0) {
    for (int i = 0; i < max_fd; i++) {
      if (is_ready(i)) {
        auto it = fds.find(i);
        it->ready = true;
        ready_cnt++;
      }
    }
  }
  return ready_cnt;
}
```
2. 使用方法
```azure
set<fd_info> fds;
while (1) {
  // Note that we need to re-initialize fds in each loop.
  fds.clear();
  fds.inert({.fd = 1})
  fds.inert({.fd = 100})

  int ready_cnt = select(fds, /*max_fd=*/100 + 1);
  assert(ready_cnt > 0);
  for (int i = 0; i < fds.size(); i++) {
    if (fds[i].ready) {
      // Use fds[i].fd
    }
  }
}

```
#### poll
  * 基于链表实现，没有最大连接数限制
  * 需要将被监听的fd列表从用户空间拷贝到内核空间
  * 轮询，复杂度o(n),n = len(fd_list),例如fd列表为1，2， 1000，复杂度为o(3)
1. 基本原理
```azure
// Returns true if fd is ready for I/O.
bool is_ready(int fd);

struct fd_info {
  int fd;
  bool ready;
};

int poll(struct fd_info* fds, int nfds) {
  int ready_cnt = 0;
  while(ready_cnt == 0) {
    for (int i = 0; i < nfds; i++) {
      if (is_ready(fds[i])) {
        fds[i].ready = true;
        ready_cnt++;
      } else {
        fds[i] = false;
      }
    }
  }
  return ready_cnt;
}
```
2. 使用方法
```azure
// Only need to initialize fds once.
fd_info fds[2];
fds[0].fd = 1;
fds[1].fd = 100;

int nfds = 2;

while (1) {
  int ready_cnt = poll(fds, nfds);
  assert(ready_cnt > 0);
  for (int i = 0; i < nfds; i++) {
    if (fds[i].ready) {
      // Use fds[i].fd
    }
  }
}
```

### epoll
* 红黑树维护被监听的fd列表，增删改查效率为o(log n)
* **事件驱动**，复杂度为o(ready fds),例如fd列表为1、2、1000，ready fds为2，复杂度为o(1)
* 只从内核态返回ready fds,减少用户空间内核空间数据传输

### 参考
[图解 | 深入揭秘 epoll 是如何实现 IO 多路复用的！](https://zhuanlan.zhihu.com/p/361750240)
